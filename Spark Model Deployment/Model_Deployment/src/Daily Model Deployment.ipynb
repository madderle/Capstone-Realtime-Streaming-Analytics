{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daily Model Deployment\n",
    "\n",
    "1. Load Model from S3\n",
    "2. Create Dataframe from structured streaming from Kinesis in 10 min intervals\n",
    "3. Make predictions\n",
    "4. Log predictions to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Initialize ####################################\n",
    "\n",
    "# Basic\n",
    "import subprocess\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import date, datetime\n",
    "import boto3\n",
    "import boto3.s3\n",
    "import os.path\n",
    "import sys\n",
    "import io\n",
    "import warnings\n",
    "\n",
    "# Pipeline\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "\n",
    "# Feature Engineering\n",
    "from pyspark.ml.feature import (VectorAssembler,VectorIndexer,\n",
    "                                Tokenizer,StopWordsRemover, CountVectorizer,IDF,StringIndexer, HashingTF)\n",
    "from pyspark.sql.functions import length\n",
    "from pyspark.sql.functions import col, udf\n",
    "#from pyspark.sql.types import StringType\n",
    "import preprocessor as p\n",
    "from pyspark.sql.functions import dayofyear, concat_ws, collect_list, countDistinct\n",
    "from pyspark.sql.types import *\n",
    "# Models\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Streaming\n",
    "from pyspark.streaming.kinesis import KinesisUtils, InitialPositionInStream\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('nlp').getOrCreate()\n",
    "\n",
    "\n",
    "# Database Setup\n",
    "import mysql.connector\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy import inspect\n",
    "from sqlalchemy import MetaData\n",
    "from sqlalchemy import Table\n",
    "from sqlalchemy import Column\n",
    "from sqlalchemy import Integer, String, DateTime, Float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['aws', 's3', 'cp', 's3://brandyn-twitter-sentiment-analysis/Models/Daily_Stock_Prediction_latest/', './Models/Daily_Stock_Prediction_latest', '--recursive'], returncode=0)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the Model\n",
    "subprocess.run(['aws', 's3','cp','s3://brandyn-twitter-sentiment-analysis/Models/Daily_Stock_Prediction_latest/','./Models/Daily_Stock_Prediction_latest','--recursive'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deserialize Model\n",
    "model = PipelineModel.load('./Models/Daily_Stock_Prediction_latest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to read from S3 and use structured streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To bypass the no s3 file system installed.\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.amazonaws:aws-java-sdk:1.10.34,org.apache.hadoop:hadoop-aws:2.6.0 pyspark-shell'\n",
    "\n",
    "spark = SparkSession.builder.appName('nlp').getOrCreate()\n",
    "sqlContext = SQLContext(spark)\n",
    "\n",
    "hadoopConf = spark._jsc.hadoopConfiguration()\n",
    "myAccessKey = os.environ['AWS_ACCESS_KEY_ID'] \n",
    "mySecretKey = os.environ['AWS_SECRET_ACCESS_KEY']\n",
    "hadoopConf.set(\"fs.s3.impl\", \"org.apache.hadoop.fs.s3native.NativeS3FileSystem\")\n",
    "hadoopConf.set(\"fs.s3.awsAccessKeyId\", myAccessKey)\n",
    "hadoopConf.set(\"fs.s3.awsSecretAccessKey\", mySecretKey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDf = sqlContext.readStream.json(\"s3://brandyn-twitter-sentiment-analysis/Twitter2018\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[created_at: string, id_str: int, text: string, quote_count: int, reply_count: int, retweet_count: int, favorite_count: int, lang: string, user_followers_count: int, user_statuses_count: int, user_name: string, user_screen_name: string, Company: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "############################# Bring in Data ###############################\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "\n",
    "#### Twitter ####\n",
    "inputPath = \"s3://brandyn-twitter-sentiment-analysis/Twitter2018/\"\n",
    "\n",
    "# Create Schema\n",
    "twitterSchema = StructType() \\\n",
    "            .add(\"created_at\", StringType()) \\\n",
    "            .add(\"id_str\", IntegerType()) \\\n",
    "            .add(\"text\", StringType()) \\\n",
    "            .add(\"quote_count\", IntegerType()) \\\n",
    "            .add(\"reply_count\", IntegerType()) \\\n",
    "            .add(\"retweet_count\", IntegerType()) \\\n",
    "            .add(\"favorite_count\", IntegerType()) \\\n",
    "            .add(\"lang\", StringType()) \\\n",
    "            .add(\"user_followers_count\", IntegerType()) \\\n",
    "            .add(\"user_statuses_count\", IntegerType()) \\\n",
    "            .add(\"user_name\", StringType()) \\\n",
    "            .add(\"user_screen_name\", StringType()) \\\n",
    "            .add(\"Company\", StringType())\n",
    "\n",
    "# Create Dataframe\n",
    "testDf = (\n",
    "    spark.read.schema(twitterSchema).json(inputPath))\n",
    "\n",
    "display(testDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+----+-----------+-----------+-------------+--------------+----+--------------------+-------------------+---------+----------------+-------+\n",
      "|created_at|id_str|text|quote_count|reply_count|retweet_count|favorite_count|lang|user_followers_count|user_statuses_count|user_name|user_screen_name|Company|\n",
      "+----------+------+----+-----------+-----------+-------------+--------------+----+--------------------+-------------------+---------+----------------+-------+\n",
      "+----------+------+----+-----------+-----------+-------------+--------------+----+--------------------+-------------------+---------+----------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataframe\n",
    "df = (\n",
    "    spark.readStream.schema(twitterSchema).json(inputPath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = df.writeStream.outputMode(\"append\").format(\"console\").start()\n",
    "# query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'Queries with streaming sources must be executed with writeStream.start();;\\nFileSource[s3://brandyn-twitter-sentiment-analysis/Twitter2018/05/18/16]'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.6-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    319\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    321\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o65.showString.\n: org.apache.spark.sql.AnalysisException: Queries with streaming sources must be executed with writeStream.start();;\nFileSource[s3://brandyn-twitter-sentiment-analysis/Twitter2018/05/18/16]\n\tat org.apache.spark.sql.catalyst.analysis.UnsupportedOperationChecker$.org$apache$spark$sql$catalyst$analysis$UnsupportedOperationChecker$$throwError(UnsupportedOperationChecker.scala:374)\n\tat org.apache.spark.sql.catalyst.analysis.UnsupportedOperationChecker$$anonfun$checkForBatch$1.apply(UnsupportedOperationChecker.scala:37)\n\tat org.apache.spark.sql.catalyst.analysis.UnsupportedOperationChecker$$anonfun$checkForBatch$1.apply(UnsupportedOperationChecker.scala:35)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\n\tat scala.collection.immutable.List.foreach(List.scala:381)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\n\tat scala.collection.immutable.List.foreach(List.scala:381)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\n\tat scala.collection.immutable.List.foreach(List.scala:381)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\n\tat scala.collection.immutable.List.foreach(List.scala:381)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\n\tat scala.collection.immutable.List.foreach(List.scala:381)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\n\tat org.apache.spark.sql.catalyst.analysis.UnsupportedOperationChecker$.checkForBatch(UnsupportedOperationChecker.scala:35)\n\tat org.apache.spark.sql.execution.QueryExecution.assertSupported(QueryExecution.scala:51)\n\tat org.apache.spark.sql.execution.QueryExecution.withCachedData$lzycompute(QueryExecution.scala:62)\n\tat org.apache.spark.sql.execution.QueryExecution.withCachedData(QueryExecution.scala:60)\n\tat org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute(QueryExecution.scala:72)\n\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:68)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:77)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3248)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2484)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2698)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:254)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-56a55b3c8673>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'*'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"retweet_count = 0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    348\u001b[0m         \"\"\"\n\u001b[1;32m    349\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1158\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1160\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'Queries with streaming sources must be executed with writeStream.start();;\\nFileSource[s3://brandyn-twitter-sentiment-analysis/Twitter2018/05/18/16]'"
     ]
    }
   ],
   "source": [
    "df.select('*').where(\"retweet_count = 0\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## From Thinkful Course\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.functions import desc, col, window\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "import json\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To bypass the no s3 file system installed.\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.amazonaws:aws-java-sdk:1.10.34,org.apache.hadoop:hadoop-aws:2.6.0 pyspark-shell'\n",
    "\n",
    "APP_NAME = \"Test\"\n",
    "SPARK_URL = \"local[*]\"\n",
    "\n",
    "spark = SparkSession.builder.appName(APP_NAME).master(SPARK_URL).getOrCreate()\n",
    "sqlContext = SQLContext(spark)\n",
    "\n",
    "hadoopConf = spark._jsc.hadoopConfiguration()\n",
    "myAccessKey = os.environ['AWS_ACCESS_KEY_ID'] \n",
    "mySecretKey = os.environ['AWS_SECRET_ACCESS_KEY']\n",
    "hadoopConf.set(\"fs.s3.impl\", \"org.apache.hadoop.fs.s3native.NativeS3FileSystem\")\n",
    "hadoopConf.set(\"fs.s3.awsAccessKeyId\", myAccessKey)\n",
    "hadoopConf.set(\"fs.s3.awsSecretAccessKey\", mySecretKey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Twitter #### Static Read\n",
    "inputPath = \"s3://brandyn-twitter-sentiment-analysis/Twitter2018/05/*/*\"\n",
    "\n",
    "# Create Schema\n",
    "twitterSchema = StructType() \\\n",
    "            .add(\"created_at\", StringType()) \\\n",
    "            .add(\"id_str\", StringType()) \\\n",
    "            .add(\"text\", StringType()) \\\n",
    "            .add(\"quote_count\", StringType()) \\\n",
    "            .add(\"reply_count\", StringType()) \\\n",
    "            .add(\"retweet_count\", StringType()) \\\n",
    "            .add(\"favorite_count\", StringType()) \\\n",
    "            .add(\"retweeted\", StringType()) \\\n",
    "            .add(\"lang\", StringType()) \\\n",
    "            .add(\"user_name\", StringType()) \\\n",
    "            .add(\"user_followers_count\", StringType()) \\\n",
    "            .add(\"user_statuses_count\", StringType()) \\\n",
    "            .add(\"user_screen_name\", StringType()) \\\n",
    "            .add(\"Company\", StringType())\n",
    "\n",
    "# Create Dataframe\n",
    "testDf = spark.read.schema(twitterSchema).json(inputPath, multiLine=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+--------------------+-----------+-----------+-------------+--------------+---------+----+--------------------+--------------------+-------------------+----------------+-----------------+\n",
      "|          created_at|            id_str|                text|quote_count|reply_count|retweet_count|favorite_count|retweeted|lang|           user_name|user_followers_count|user_statuses_count|user_screen_name|          Company|\n",
      "+--------------------+------------------+--------------------+-----------+-----------+-------------+--------------+---------+----+--------------------+--------------------+-------------------+----------------+-----------------+\n",
      "|Fri May 18 16:18:...|997511693264719872|RT @NOD008: I've ...|          0|          0|            0|             0|    false|  en|       Hugh R Calder|                  65|               3862|     DrHugh2thDr|         [\"TSLA\"]|\n",
      "|Fri May 18 16:12:...|997510337673871360|Isn't Elon suppos...|          0|          0|            0|             0|    false|  en|          Tim Knight|               18822|              28248|     SlopeOfHope|         [\"TSLA\"]|\n",
      "|Fri May 18 16:28:...|997514218269065216|Here are the top ...|          0|          0|            0|             0|    false|  en|             The Fly|               18189|              81197|      theflynews|            [\"V\"]|\n",
      "|Fri May 18 17:57:...|997536730961018880|$GOOGL WSJ: Googl...|          0|          0|            0|             0|    false|  en|       StockNews.com|                 883|             163970| stocknewsdotcom| [\"GOOG\",\"GOOGL\"]|\n",
      "|Fri May 18 16:33:...|997515536228667392|Started small wit...|          0|          0|            0|             0|    false|  en|       Travis Howard|                1377|              16340|   TravisHoward5|    [\"BA\",\"BABA\"]|\n",
      "|Fri May 18 16:48:...|997519407466168320|$XOM 2018-05-18 m...|          0|          0|            0|             0|    false|  en|                FinX|                   7|              24465|         FinX_io|          [\"XOM\"]|\n",
      "|Fri May 18 16:23:...|997512956207337472|$TSLA STC half my...|          0|          0|            0|             0|    false|  en|       labombagrande|                  29|                590|   labombagrande|         [\"TSLA\"]|\n",
      "|Fri May 18 16:59:...|997522050653851648|Three Corner Glob...|          0|          0|            0|             0|    false|  en|Money Making Arti...|                  17|               9347|    mmahotstuff1|            [\"C\"]|\n",
      "|Fri May 18 17:26:...|997528861423427584|$WFC Wells Fargo ...|          0|          0|            0|             0|    false|  en|          StockTexts|                 557|             153161|      StockTexts|          [\"WFC\"]|\n",
      "|Fri May 18 16:07:...|997509042477129731|RT @TommyThornton...|          0|          0|            0|             0|    false|  en|     David Bergerson|                 123|              19603|      bergie1393|         [\"TSLA\"]|\n",
      "|Fri May 18 18:13:...|997540620938948609|5.18.18 Elliott W...|          0|          0|            0|             0|    false|  en|          Ted Aguhob|                1592|              33097|      wavegenius|[\"BA\",\"V\",\"BABA\"]|\n",
      "|Fri May 18 17:10:...|997524912943312896|Hutchinson Capita...|          0|          0|            0|             0|    false|  en|   The Norman Weekly|                  11|               6301|    normanweekly|          [\"XOM\"]|\n",
      "|Fri May 18 18:18:...|997541925065551873|$JPM getting into...|          0|          0|            0|             0|    false|  en| SmallCapReporter ðŸ“ˆ|                3669|               6272|      MrSmallCap|          [\"JPM\"]|\n",
      "|Fri May 18 19:00:...|997552407239446528|Fake event for $A...|          0|          0|            0|             0|    false|  en|                 yon|                   1|                 68| yon_yon_yon_yon|         [\"AAPL\"]|\n",
      "|Fri May 18 18:43:...|997548338328293376|Warren Averett As...|          0|          0|            0|             0|    false|  en|      WhatsOnThorold|                  19|              11213| whatsonthorold2|         [\"INTC\"]|\n",
      "|Fri May 18 16:43:...|997518100030480384|American Asset Ma...|          0|          0|            0|             0|    false|  en|         Utah Herald|                  14|              11992|      utahherald|           [\"GE\"]|\n",
      "|Fri May 18 17:52:...|997535474603515905|Frontier Investme...|          0|          0|            0|             0|    false|  en|           Herald KS|                  27|              13909|        heraldks|      [\"XOM\",\"C\"]|\n",
      "|Fri May 18 17:47:...|997534153523499008|https://t.co/uAca...|          0|          0|            0|             0|    false|  en|          Bilo Selhi|               24604|              79312| StocksThatDoubl|            [\"C\"]|\n",
      "|Fri May 18 18:28:...|997544475600842752|See why these ass...|          0|          0|            0|             0|    false|  en|             FinBuzz|                1152|             238481|   PortfolioBuzz|         [\"TSLA\"]|\n",
      "|Fri May 18 18:33:...|997545755329155072|Nicholas Investme...|          0|          0|            0|             0|    false|  en|          Bibey Post|                  13|               8153| bibeypost_stock|      [\"PFE\",\"C\"]|\n",
      "+--------------------+------------------+--------------------+-----------+-----------+-------------+--------------+---------+----+--------------------+--------------------+-------------------+----------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Streaming Input Dataframe\n",
    "from pyspark.streaming import StreamingContext\n",
    "streamingInputDF = (\n",
    "  spark\n",
    "    .readStream                       \n",
    "    .schema(twitterSchema)               # Set the schema of the JSON data\n",
    "    .option(\"maxFilesPerTrigger\", 1)  # Treat a sequence of files as a stream by picking one file at a time\n",
    "    .json(inputPath)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create STreaming dataframe\n",
    "streamingWindowDF = streamingInputDF \\\n",
    "    .select('*')\n",
    "    .groupBy\n",
    "\n",
    "# Is this Streaming?\n",
    "streamingWindowDF.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'Complete output mode not supported when there are no streaming aggregations on streaming DataFrames/Datasets;;\\nProject [created_at#944, id_str#945, text#946, quote_count#947, reply_count#948, retweet_count#949, favorite_count#950, retweeted#951, lang#952, user_name#953, user_followers_count#954, user_statuses_count#955, user_screen_name#956, Company#957]\\n+- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@4cb89750,json,List(),Some(StructType(StructField(created_at,StringType,true), StructField(id_str,StringType,true), StructField(text,StringType,true), StructField(quote_count,StringType,true), StructField(reply_count,StringType,true), StructField(retweet_count,StringType,true), StructField(favorite_count,StringType,true), StructField(retweeted,StringType,true), StructField(lang,StringType,true), StructField(user_name,StringType,true), StructField(user_followers_count,StringType,true), StructField(user_statuses_count,StringType,true), StructField(user_screen_name,StringType,true), StructField(Company,StringType,true))),List(),None,Map(maxFilesPerTrigger -> 1, path -> s3://brandyn-twitter-sentiment-analysis/Twitter2018/05/*/*),None), FileSource[s3://brandyn-twitter-sentiment-analysis/Twitter2018/05/*/*], [created_at#944, id_str#945, text#946, quote_count#947, reply_count#948, retweet_count#949, favorite_count#950, retweeted#951, lang#952, user_name#953, user_followers_count#954, user_statuses_count#955, user_screen_name#956, Company#957]\\n'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.6-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    319\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    321\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o317.start.\n: org.apache.spark.sql.AnalysisException: Complete output mode not supported when there are no streaming aggregations on streaming DataFrames/Datasets;;\nProject [created_at#944, id_str#945, text#946, quote_count#947, reply_count#948, retweet_count#949, favorite_count#950, retweeted#951, lang#952, user_name#953, user_followers_count#954, user_statuses_count#955, user_screen_name#956, Company#957]\n+- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@4cb89750,json,List(),Some(StructType(StructField(created_at,StringType,true), StructField(id_str,StringType,true), StructField(text,StringType,true), StructField(quote_count,StringType,true), StructField(reply_count,StringType,true), StructField(retweet_count,StringType,true), StructField(favorite_count,StringType,true), StructField(retweeted,StringType,true), StructField(lang,StringType,true), StructField(user_name,StringType,true), StructField(user_followers_count,StringType,true), StructField(user_statuses_count,StringType,true), StructField(user_screen_name,StringType,true), StructField(Company,StringType,true))),List(),None,Map(maxFilesPerTrigger -> 1, path -> s3://brandyn-twitter-sentiment-analysis/Twitter2018/05/*/*),None), FileSource[s3://brandyn-twitter-sentiment-analysis/Twitter2018/05/*/*], [created_at#944, id_str#945, text#946, quote_count#947, reply_count#948, retweet_count#949, favorite_count#950, retweeted#951, lang#952, user_name#953, user_followers_count#954, user_statuses_count#955, user_screen_name#956, Company#957]\n\n\tat org.apache.spark.sql.catalyst.analysis.UnsupportedOperationChecker$.org$apache$spark$sql$catalyst$analysis$UnsupportedOperationChecker$$throwError(UnsupportedOperationChecker.scala:374)\n\tat org.apache.spark.sql.catalyst.analysis.UnsupportedOperationChecker$.checkForStreaming(UnsupportedOperationChecker.scala:116)\n\tat org.apache.spark.sql.streaming.StreamingQueryManager.createQuery(StreamingQueryManager.scala:235)\n\tat org.apache.spark.sql.streaming.StreamingQueryManager.startQuery(StreamingQueryManager.scala:299)\n\tat org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:258)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-b3fd9e0e3092>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mqueryName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"stocks\"\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;31m# counts = name of the in-memory table\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;34m.\u001b[0m\u001b[0moutputMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"complete\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# complete = all the counts should be in the table\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m )\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/streaming.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self, path, format, outputMode, partitionBy, queryName, **options)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueryName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueryName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1158\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1160\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'Complete output mode not supported when there are no streaming aggregations on streaming DataFrames/Datasets;;\\nProject [created_at#944, id_str#945, text#946, quote_count#947, reply_count#948, retweet_count#949, favorite_count#950, retweeted#951, lang#952, user_name#953, user_followers_count#954, user_statuses_count#955, user_screen_name#956, Company#957]\\n+- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@4cb89750,json,List(),Some(StructType(StructField(created_at,StringType,true), StructField(id_str,StringType,true), StructField(text,StringType,true), StructField(quote_count,StringType,true), StructField(reply_count,StringType,true), StructField(retweet_count,StringType,true), StructField(favorite_count,StringType,true), StructField(retweeted,StringType,true), StructField(lang,StringType,true), StructField(user_name,StringType,true), StructField(user_followers_count,StringType,true), StructField(user_statuses_count,StringType,true), StructField(user_screen_name,StringType,true), StructField(Company,StringType,true))),List(),None,Map(maxFilesPerTrigger -> 1, path -> s3://brandyn-twitter-sentiment-analysis/Twitter2018/05/*/*),None), FileSource[s3://brandyn-twitter-sentiment-analysis/Twitter2018/05/*/*], [created_at#944, id_str#945, text#946, quote_count#947, reply_count#948, retweet_count#949, favorite_count#950, retweeted#951, lang#952, user_name#953, user_followers_count#954, user_statuses_count#955, user_screen_name#956, Company#957]\\n'"
     ]
    }
   ],
   "source": [
    "# Now start the engine\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"2\")  # keep the size of shuffles small\n",
    "\n",
    "# Write stream to an in memroy table called \n",
    "query = (\n",
    "  streamingWindowDF\n",
    "    .writeStream\n",
    "    .format(\"memory\")       \n",
    "    .queryName(\"stocks\")     # counts = name of the in-memory table\n",
    "    .outputMode(\"complete\")  # complete = all the counts should be in the table\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
