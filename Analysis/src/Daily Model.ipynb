{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daily Model Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Initialize ###################\n",
    "\n",
    "# Basics\n",
    "from pymongo import MongoClient\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import boto3\n",
    "import io\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# NLP\n",
    "import nltk\n",
    "import spacy\n",
    "spacy.load('en')\n",
    "from nltk.corpus import stopwords\n",
    "import preprocessor as p\n",
    "\n",
    "# Model Infrastructure\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD, PCA\n",
    "from sklearn.pipeline import make_pipeline, Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "\n",
    "\n",
    "# Models\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import linear_model\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier, XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### Bring In Data #######################\n",
    "#Setup Mongo and create the database and collection\n",
    "User = os.environ['MONGODB_USER']\n",
    "password = os.environ['MONGODB_PASS']\n",
    "IP = os.environ['IP']\n",
    "\n",
    "client = MongoClient(IP, username=User, password=password)\n",
    "db = client['stock_tweets']\n",
    "\n",
    "#Grab references\n",
    "twitter_coll_reference = db.twitter\n",
    "iex_coll_reference = db.iex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Execution time: 32.13582253456116 seconds ---\n"
     ]
    }
   ],
   "source": [
    "###################### Build Twitter Data Frames #####################\n",
    "\n",
    "start_time = time.time()\n",
    "# Create Data Frame\n",
    "twitter_data = pd.DataFrame(list(twitter_coll_reference.find()))\n",
    "\n",
    "# Need to convert the created_at to a time stamp and set to index\n",
    "twitter_data.index=pd.to_datetime(twitter_data['created_at'])\n",
    "\n",
    "# Delimited the Company List into separate rows\n",
    "delimited_twitter_data=[]\n",
    "\n",
    "for item in twitter_data.itertuples():\n",
    "    #twitter_dict={}\n",
    "    for company in item[1]:\n",
    "        twitter_dict={}\n",
    "        twitter_dict['created_at']=item[0]\n",
    "        twitter_dict['company']=company\n",
    "        twitter_dict['text']=item[11]\n",
    "        twitter_dict['user_followers_count']=item[12]\n",
    "        twitter_dict['user_name']=item[13]\n",
    "        twitter_dict['user_statuses_count']=item[15]\n",
    "        delimited_twitter_data.append(twitter_dict)\n",
    "\n",
    "delimited_twitter_df = pd.DataFrame(delimited_twitter_data) \n",
    "delimited_twitter_df.set_index('created_at', inplace=True)\n",
    "\n",
    "# Create hourly data frame\n",
    "twitter_delimited_hourly = delimited_twitter_df.groupby([pd.Grouper(freq=\"D\"), 'company']).count()['text'].to_frame()\n",
    "twitter_delimited_hourly.columns = ['Number_of_Tweets']\n",
    "\n",
    "# Concatenate the text with a space to not combine words.\n",
    "twitter_delimited_hourly['text']=delimited_twitter_df.groupby([pd.Grouper(freq=\"D\"), 'company'])['text'].apply(lambda x: ' '.join(x))\n",
    "# Number of Users\n",
    "twitter_delimited_hourly['Number_of_Users'] = delimited_twitter_df.groupby([pd.Grouper(freq=\"D\"), 'company'])['user_name'].nunique()\n",
    "\n",
    "# Rename Index\n",
    "twitter_delimited_hourly = twitter_delimited_hourly.reindex(twitter_delimited_hourly.index.rename(['Time', 'Company']))\n",
    "\n",
    "print(\"-- Execution time: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Execution time: 9.874674320220947 seconds ---\n"
     ]
    }
   ],
   "source": [
    "##################### Build Stock Data Frames ###########################\n",
    "start_time = time.time()\n",
    "\n",
    "stock_data = pd.DataFrame(list(iex_coll_reference.find()))\n",
    "\n",
    "# Need to convert the created_at to a time stamp\n",
    "stock_data.index=pd.to_datetime(stock_data['latestUpdate'])\n",
    "stock_data['latestUpdate'] = pd.to_datetime(stock_data['latestUpdate'])\n",
    "#Group By hourly and stock price\n",
    "# Need to get the first stock price in teh hour, and then the last to take the difference to see how much change.\n",
    "stock_delimited_hourly = stock_data.sort_values('latestUpdate').groupby([pd.Grouper(freq=\"D\"), 'Ticker']).first()['latestPrice'].to_frame()\n",
    "stock_delimited_hourly.columns = ['First_Price']\n",
    "stock_delimited_hourly['Last_Price'] = stock_data.sort_values('latestUpdate').groupby([pd.Grouper(freq=\"D\"), 'Ticker']).last()['latestPrice']\n",
    "\n",
    "# Then need to take the difference and turn into a percentage.\n",
    "stock_delimited_hourly['Price_Percent_Change'] = ((stock_delimited_hourly['Last_Price'] \n",
    "                                                   - stock_delimited_hourly['First_Price'])/stock_delimited_hourly['First_Price'])*100\n",
    "\n",
    "# Need to also show Percent from open price\n",
    "stock_delimited_hourly['Open_Price'] = stock_data.groupby([pd.Grouper(freq=\"D\"), 'Ticker'])['open'].mean()\n",
    "stock_delimited_hourly['Price_Percent_Open'] = ((stock_delimited_hourly['Last_Price'] \n",
    "                                                 - stock_delimited_hourly['Open_Price'])/stock_delimited_hourly['Open_Price'])*100\n",
    "\n",
    "# Also include mean volume\n",
    "stock_delimited_hourly['Mean_Volume'] = stock_data.groupby([pd.Grouper(freq=\"D\"), 'Ticker'])['latestVolume'].mean()\n",
    "\n",
    "# Classification Labels\n",
    "stock_delimited_hourly['Price_Change'] = np.where(stock_delimited_hourly['Price_Percent_Change']>=0, 1, 0)\n",
    "stock_delimited_hourly['Open_Price_Change'] = np.where(stock_delimited_hourly['Price_Percent_Open']>=0, 1, 0)\n",
    "\n",
    "# Rename the Index\n",
    "stock_delimited_hourly = stock_delimited_hourly.reindex(stock_delimited_hourly.index.rename(['Time', 'Company']))\n",
    "print(\"-- Execution time: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Number_of_Tweets</th>\n",
       "      <th>text</th>\n",
       "      <th>Number_of_Users</th>\n",
       "      <th>First_Price</th>\n",
       "      <th>Last_Price</th>\n",
       "      <th>Price_Percent_Change</th>\n",
       "      <th>Open_Price</th>\n",
       "      <th>Price_Percent_Open</th>\n",
       "      <th>Mean_Volume</th>\n",
       "      <th>Price_Change</th>\n",
       "      <th>Open_Price_Change</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Time</th>\n",
       "      <th>Company</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2018-03-12</th>\n",
       "      <th>AAPL</th>\n",
       "      <td>403</td>\n",
       "      <td>@JoKiddo But how proprietary is that? Does it ...</td>\n",
       "      <td>258</td>\n",
       "      <td>181.730</td>\n",
       "      <td>181.75</td>\n",
       "      <td>0.011005</td>\n",
       "      <td>180.23</td>\n",
       "      <td>0.843367</td>\n",
       "      <td>2.767373e+07</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AMZN</th>\n",
       "      <td>275</td>\n",
       "      <td>Amazon hits $1600 $AMZN Americans reported one...</td>\n",
       "      <td>162</td>\n",
       "      <td>1600.745</td>\n",
       "      <td>1598.39</td>\n",
       "      <td>-0.147119</td>\n",
       "      <td>1592.60</td>\n",
       "      <td>0.363556</td>\n",
       "      <td>4.376277e+06</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BA</th>\n",
       "      <td>137</td>\n",
       "      <td>Thus, its cheaper for $AAPL to built than to b...</td>\n",
       "      <td>94</td>\n",
       "      <td>345.910</td>\n",
       "      <td>344.19</td>\n",
       "      <td>-0.497239</td>\n",
       "      <td>355.02</td>\n",
       "      <td>-3.050532</td>\n",
       "      <td>5.150044e+06</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BABA</th>\n",
       "      <td>50</td>\n",
       "      <td>Thus, its cheaper for $AAPL to built than to b...</td>\n",
       "      <td>37</td>\n",
       "      <td>192.900</td>\n",
       "      <td>192.74</td>\n",
       "      <td>-0.082945</td>\n",
       "      <td>192.00</td>\n",
       "      <td>0.385417</td>\n",
       "      <td>1.622245e+07</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BAC</th>\n",
       "      <td>51</td>\n",
       "      <td>Open an account with @RobinhoodApp and get a s...</td>\n",
       "      <td>35</td>\n",
       "      <td>32.980</td>\n",
       "      <td>32.84</td>\n",
       "      <td>-0.424500</td>\n",
       "      <td>32.67</td>\n",
       "      <td>0.520355</td>\n",
       "      <td>4.670738e+07</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Number_of_Tweets  \\\n",
       "Time       Company                     \n",
       "2018-03-12 AAPL                  403   \n",
       "           AMZN                  275   \n",
       "           BA                    137   \n",
       "           BABA                   50   \n",
       "           BAC                    51   \n",
       "\n",
       "                                                                 text  \\\n",
       "Time       Company                                                      \n",
       "2018-03-12 AAPL     @JoKiddo But how proprietary is that? Does it ...   \n",
       "           AMZN     Amazon hits $1600 $AMZN Americans reported one...   \n",
       "           BA       Thus, its cheaper for $AAPL to built than to b...   \n",
       "           BABA     Thus, its cheaper for $AAPL to built than to b...   \n",
       "           BAC      Open an account with @RobinhoodApp and get a s...   \n",
       "\n",
       "                    Number_of_Users  First_Price  Last_Price  \\\n",
       "Time       Company                                             \n",
       "2018-03-12 AAPL                 258      181.730      181.75   \n",
       "           AMZN                 162     1600.745     1598.39   \n",
       "           BA                    94      345.910      344.19   \n",
       "           BABA                  37      192.900      192.74   \n",
       "           BAC                   35       32.980       32.84   \n",
       "\n",
       "                    Price_Percent_Change  Open_Price  Price_Percent_Open  \\\n",
       "Time       Company                                                         \n",
       "2018-03-12 AAPL                 0.011005      180.23            0.843367   \n",
       "           AMZN                -0.147119     1592.60            0.363556   \n",
       "           BA                  -0.497239      355.02           -3.050532   \n",
       "           BABA                -0.082945      192.00            0.385417   \n",
       "           BAC                 -0.424500       32.67            0.520355   \n",
       "\n",
       "                     Mean_Volume  Price_Change  Open_Price_Change  \n",
       "Time       Company                                                 \n",
       "2018-03-12 AAPL     2.767373e+07             1                  1  \n",
       "           AMZN     4.376277e+06             0                  1  \n",
       "           BA       5.150044e+06             0                  0  \n",
       "           BABA     1.622245e+07             0                  1  \n",
       "           BAC      4.670738e+07             0                  1  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######################### Combine Data Frames ##############################\n",
    "daily_df = pd.concat([twitter_delimited_hourly, stock_delimited_hourly], axis=1, join='inner')\n",
    "daily_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To flatten after combined everything. \n",
    "daily_df.reset_index(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Execution time: 4.336963415145874 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# Clean the Tweets\n",
    "p.set_options(p.OPT.URL, p.OPT.EMOJI, p.OPT.MENTION, p.OPT.RESERVED, p.OPT.EMOJI, p.OPT.HASHTAG)\n",
    "def preprocess_tweet(tweet):\n",
    "    return p.clean(tweet)\n",
    "\n",
    "# Clean the tweets, by removing special characters\n",
    "start_time = time.time()\n",
    "daily_df['Clean_text'] = hourly_df['text'].apply(lambda x: preprocess_tweet(x))\n",
    "print(\"-- Execution time: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Between Outcome and Features\n",
    "features = daily_df[['Company','Number_of_Tweets', 'Number_of_Users','Mean_Volume','Clean_text']]\n",
    "classification_price = daily_df['Price_Change']\n",
    "classification_open = daily_df['Open_Price_Change']\n",
    "regression_price = daily_df['Price_Percent_Change']\n",
    "regression_open = daily_df['Price_Percent_Open']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number_of_Tweets</th>\n",
       "      <th>Number_of_Users</th>\n",
       "      <th>Mean_Volume</th>\n",
       "      <th>Clean_text</th>\n",
       "      <th>Company_AMZN</th>\n",
       "      <th>Company_BA</th>\n",
       "      <th>Company_BABA</th>\n",
       "      <th>Company_BAC</th>\n",
       "      <th>Company_C</th>\n",
       "      <th>Company_CMCSA</th>\n",
       "      <th>...</th>\n",
       "      <th>Company_NFLX</th>\n",
       "      <th>Company_NVDA</th>\n",
       "      <th>Company_PFE</th>\n",
       "      <th>Company_TSLA</th>\n",
       "      <th>Company_TWTR</th>\n",
       "      <th>Company_UNH</th>\n",
       "      <th>Company_UTX</th>\n",
       "      <th>Company_V</th>\n",
       "      <th>Company_WFC</th>\n",
       "      <th>Company_XOM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>403</td>\n",
       "      <td>258</td>\n",
       "      <td>2.767373e+07</td>\n",
       "      <td>But how proprietary is that? Does it really ma...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>275</td>\n",
       "      <td>162</td>\n",
       "      <td>4.376277e+06</td>\n",
       "      <td>Amazon hits $1600 $AMZN Americans reported one...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>137</td>\n",
       "      <td>94</td>\n",
       "      <td>5.150044e+06</td>\n",
       "      <td>Thus, its cheaper for $AAPL to built than to b...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50</td>\n",
       "      <td>37</td>\n",
       "      <td>1.622245e+07</td>\n",
       "      <td>Thus, its cheaper for $AAPL to built than to b...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>51</td>\n",
       "      <td>35</td>\n",
       "      <td>4.670738e+07</td>\n",
       "      <td>Open an account with and get a stock like $HPQ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Number_of_Tweets  Number_of_Users   Mean_Volume  \\\n",
       "0               403              258  2.767373e+07   \n",
       "1               275              162  4.376277e+06   \n",
       "2               137               94  5.150044e+06   \n",
       "3                50               37  1.622245e+07   \n",
       "4                51               35  4.670738e+07   \n",
       "\n",
       "                                          Clean_text  Company_AMZN  \\\n",
       "0  But how proprietary is that? Does it really ma...             0   \n",
       "1  Amazon hits $1600 $AMZN Americans reported one...             1   \n",
       "2  Thus, its cheaper for $AAPL to built than to b...             0   \n",
       "3  Thus, its cheaper for $AAPL to built than to b...             0   \n",
       "4  Open an account with and get a stock like $HPQ...             0   \n",
       "\n",
       "   Company_BA  Company_BABA  Company_BAC  Company_C  Company_CMCSA  \\\n",
       "0           0             0            0          0              0   \n",
       "1           0             0            0          0              0   \n",
       "2           1             0            0          0              0   \n",
       "3           0             1            0          0              0   \n",
       "4           0             0            1          0              0   \n",
       "\n",
       "      ...       Company_NFLX  Company_NVDA  Company_PFE  Company_TSLA  \\\n",
       "0     ...                  0             0            0             0   \n",
       "1     ...                  0             0            0             0   \n",
       "2     ...                  0             0            0             0   \n",
       "3     ...                  0             0            0             0   \n",
       "4     ...                  0             0            0             0   \n",
       "\n",
       "   Company_TWTR  Company_UNH  Company_UTX  Company_V  Company_WFC  Company_XOM  \n",
       "0             0            0            0          0            0            0  \n",
       "1             0            0            0          0            0            0  \n",
       "2             0            0            0          0            0            0  \n",
       "3             0            0            0          0            0            0  \n",
       "4             0            0            0          0            0            0  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Want to leverage the Company name so need to create dummy variables. \n",
    "cat_feats = ['Company']\n",
    "features = pd.get_dummies(features, columns=cat_feats, drop_first=True)\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will create pipeline that combines all the steps and then apply the model at the end.\n",
    "# Since all the features are apart of the features dataframe, for the NLP only need the clean text, but still want to add other things.\n",
    "\n",
    "# Will create a class to handle this. \n",
    "class DataSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "        \n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, features):\n",
    "        if self.key=='text':\n",
    "            return features['Clean_text']\n",
    "        else:\n",
    "            return features.loc[:, features.columns != 'Clean_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Price Change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    661\n",
       "1    527\n",
       "Name: Price_Change, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check is Data is imbalanced\n",
    "daily_df['Price_Change'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the Data to avoid Leakage\n",
    "#splitting into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features,classification_price,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing Finished. Number of features: 152113\n",
      "Explained Variance: 0.9692198227304989\n",
      "-- Execution time: 70.19580101966858 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# Check to see how much variance is being explained\n",
    "# 152113 Features\n",
    "\n",
    "# 500 Components Explains 0.858 in 54 seconds\n",
    "# 1000 components explains 1.0 in 88 seconds\n",
    "# 750 components explains 0.969 in 70 seconds\n",
    "X = X_train['Clean_text']\n",
    "\n",
    "start_time = time.time()\n",
    "# Create lemmatizer using spacy\n",
    "lemmatizer = spacy.lang.en.English()\n",
    "\n",
    "def custom_tokenizer(doc):\n",
    "    tokens = lemmatizer(doc)\n",
    "    return([token.lemma_ for token in tokens if not token.is_punct])\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer=custom_tokenizer, stop_words='english',\n",
    "                             lowercase=True, use_idf=True, max_df=0.5, \n",
    "                             min_df=2, norm='l2', smooth_idf=True, ngram_range=(1, 2))\n",
    "\n",
    "tweets_tfidf = vectorizer.fit_transform(X)\n",
    "print(\"Vectorizing Finished. Number of features: %d\" % tweets_tfidf.get_shape()[1])\n",
    "pipe = Pipeline(steps=[\n",
    "                 ('svd', TruncatedSVD(750)),\n",
    "                 ('norm',Normalizer(copy=False))\n",
    "                       ])\n",
    "\n",
    "pipe.fit_transform(tweets_tfidf)\n",
    "print(\"Explained Variance: \" + str(pipe.get_params()['svd'].explained_variance_ratio_.sum()))\n",
    "print(\"-- Execution time: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Execution time: 184.98584127426147 seconds ---\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.65      0.73      0.69       111\n",
      "          1       0.73      0.65      0.69       127\n",
      "\n",
      "avg / total       0.69      0.69      0.69       238\n",
      "\n"
     ]
    }
   ],
   "source": [
    "####### Logistic Regression ############\n",
    "\n",
    "start_time = time.time()\n",
    "# Create lemmatizer using spacy\n",
    "lemmatizer = spacy.lang.en.English()\n",
    "\n",
    "# Define Model\n",
    "lr_model = LogisticRegression()\n",
    "\n",
    "# Define custom Tokenizer\n",
    "def custom_tokenizer(doc):\n",
    "    tokens = lemmatizer(doc)\n",
    "    return([token.lemma_ for token in tokens if not token.is_punct])\n",
    "\n",
    "# Define Vectorizer\n",
    "vectorizer = TfidfVectorizer(tokenizer=custom_tokenizer, stop_words='english',\n",
    "                             lowercase=True, use_idf=True, max_df=2,\n",
    "                             min_df=2, norm='l2', smooth_idf=True, ngram_range=(1, 2))\n",
    "\n",
    "# Define Pipeline and Feature Union\n",
    "pipeline = Pipeline([\n",
    "    # Use Feature Union to combine features from the Tweet and other features gathered\n",
    "    ('union', FeatureUnion(\n",
    "        transformer_list=[\n",
    "            # Pipeline for text\n",
    "            ('tweet', Pipeline([\n",
    "                ('selector', DataSelector(key='text')),\n",
    "                ('vectidf', vectorizer),\n",
    "                ('svd', TruncatedSVD(1000)),\n",
    "                ('norm',Normalizer(copy=False))\n",
    "                                \n",
    "            ])),\n",
    "            \n",
    "            # Pipeline for getting other features\n",
    "            ('other', Pipeline([\n",
    "                ('seclector', DataSelector(key='other'))\n",
    "             ])),\n",
    "        ],\n",
    "                       \n",
    "    )),\n",
    "    # Use Logistic Regression Classifier\n",
    "    ('lr', lr_model)\n",
    "])\n",
    "\n",
    "# Grid Search\n",
    "parameters = {\n",
    "                'lr__penalty':['l1'],\n",
    "                'lr__C':[10],\n",
    "                'lr__class_weight':['balanced'],\n",
    "                'union__transformer_weights':[{'tweet':0.7, 'other':0.3},{'tweet':0.8, 'other':0.2}]\n",
    "               \n",
    "              }\n",
    "\n",
    "grid = GridSearchCV(pipeline, parameters, scoring='accuracy', cv=3, verbose=0, n_jobs=5)\n",
    "\n",
    "# Fit the grid\n",
    "grid.fit(X_train, y_train)\n",
    "# Predictions\n",
    "y = grid.predict(X_test)\n",
    "print(\"-- Execution time: %s seconds ---\" % (time.time() - start_time))\n",
    "print(classification_report(y, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lr__C': 10,\n",
       " 'lr__class_weight': 'balanced',\n",
       " 'lr__penalty': 'l1',\n",
       " 'union__transformer_weights': {'other': 0.2, 'tweet': 0.8}}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Execution time: 163.80610704421997 seconds ---\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.74      0.55      0.63       167\n",
      "          1       0.34      0.54      0.41        71\n",
      "\n",
      "avg / total       0.62      0.55      0.57       238\n",
      "\n"
     ]
    }
   ],
   "source": [
    "####### Random Forest ############\n",
    "\n",
    "start_time = time.time()\n",
    "# Create lemmatizer using spacy\n",
    "lemmatizer = spacy.lang.en.English()\n",
    "\n",
    "# Define Model\n",
    "rfc_model = RandomForestClassifier(n_jobs=10)\n",
    "\n",
    "# Define custom Tokenizer\n",
    "def custom_tokenizer(doc):\n",
    "    tokens = lemmatizer(doc)\n",
    "    return([token.lemma_ for token in tokens if not token.is_punct])\n",
    "\n",
    "# Define Vectorizer\n",
    "vectorizer = TfidfVectorizer(tokenizer=custom_tokenizer, stop_words='english',\n",
    "                             lowercase=True, use_idf=True, max_df=2,\n",
    "                             min_df=2, norm='l2', smooth_idf=True, ngram_range=(1, 2))\n",
    "\n",
    "# Define Pipeline and Feature Union\n",
    "pipeline = Pipeline([\n",
    "    # Use Feature Union to combine features from the Tweet and other features gathered\n",
    "    ('union', FeatureUnion(\n",
    "        transformer_list=[\n",
    "            # Pipeline for text\n",
    "            ('tweet', Pipeline([\n",
    "                ('selector', DataSelector(key='text')),\n",
    "                ('vectidf', vectorizer),\n",
    "                ('svd', TruncatedSVD(1000)),\n",
    "                ('norm',Normalizer(copy=False))\n",
    "                                \n",
    "            ])),\n",
    "            \n",
    "            # Pipeline for getting other features\n",
    "            ('other', Pipeline([\n",
    "                ('seclector', DataSelector(key='other'))\n",
    "             ])),\n",
    "        ],\n",
    "                       \n",
    "    )),\n",
    "    # Use Logistic Regression Classifier\n",
    "    ('rfc', rfc_model)\n",
    "])\n",
    "\n",
    "# Grid Search\n",
    "parameters = {\n",
    "                \n",
    "                'rfc__class_weight':['balanced'],\n",
    "                'union__transformer_weights':[{'tweet':0.5, 'other':0.5},{'tweet':0.2, 'other':0.8},{'tweet':0.8, 'other':0.2}]\n",
    "               \n",
    "              }\n",
    "\n",
    "grid = GridSearchCV(pipeline, parameters, scoring='accuracy', cv=3, verbose=0, n_jobs=5)\n",
    "\n",
    "# Fit the grid\n",
    "grid.fit(X_train, y_train)\n",
    "# Predictions\n",
    "y = grid.predict(X_test)\n",
    "print(\"-- Execution time: %s seconds ---\" % (time.time() - start_time))\n",
    "print(classification_report(y, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rfc__class_weight': 'balanced',\n",
       " 'union__transformer_weights': {'other': 0.5, 'tweet': 0.5}}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Execution time: 162.1376187801361 seconds ---\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00         0\n",
      "          1       1.00      0.47      0.64       238\n",
      "\n",
      "avg / total       1.00      0.47      0.64       238\n",
      "\n"
     ]
    }
   ],
   "source": [
    "####### SGD ############\n",
    "\n",
    "start_time = time.time()\n",
    "# Create lemmatizer using spacy\n",
    "lemmatizer = spacy.lang.en.English()\n",
    "\n",
    "# Define Model\n",
    "sgd_model = linear_model.SGDClassifier(n_jobs=10)\n",
    "\n",
    "# Define custom Tokenizer\n",
    "def custom_tokenizer(doc):\n",
    "    tokens = lemmatizer(doc)\n",
    "    return([token.lemma_ for token in tokens if not token.is_punct])\n",
    "\n",
    "# Define Vectorizer\n",
    "vectorizer = TfidfVectorizer(tokenizer=custom_tokenizer, stop_words='english',\n",
    "                             lowercase=True, use_idf=True, max_df=2,\n",
    "                             min_df=2, norm='l2', smooth_idf=True, ngram_range=(1, 2))\n",
    "\n",
    "# Define Pipeline and Feature Union\n",
    "pipeline = Pipeline([\n",
    "    # Use Feature Union to combine features from the Tweet and other features gathered\n",
    "    ('union', FeatureUnion(\n",
    "        transformer_list=[\n",
    "            # Pipeline for text\n",
    "            ('tweet', Pipeline([\n",
    "                ('selector', DataSelector(key='text')),\n",
    "                ('vectidf', vectorizer),\n",
    "                ('svd', TruncatedSVD(1000)),\n",
    "                ('norm',Normalizer(copy=False))\n",
    "                                \n",
    "            ])),\n",
    "            \n",
    "            # Pipeline for getting other features\n",
    "            ('other', Pipeline([\n",
    "                ('seclector', DataSelector(key='other'))\n",
    "             ])),\n",
    "        ],\n",
    "                       \n",
    "    )),\n",
    "    # Use Logistic Regression Classifier\n",
    "    ('sgd', sgd_model)\n",
    "])\n",
    "\n",
    "# Grid Search\n",
    "parameters = {\n",
    "                \n",
    "                'sgd__loss':['log'],\n",
    "                'sgd__penalty':['l2'],\n",
    "                'sgd__fit_intercept':[True],\n",
    "                'sgd__class_weight':['balanced'],\n",
    "                'union__transformer_weights':[{'tweet':0.5, 'other':0.5},{'tweet':0.2, 'other':0.8},{'tweet':0.8, 'other':0.2}]\n",
    "               \n",
    "              }\n",
    "\n",
    "grid = GridSearchCV(pipeline, parameters, scoring='accuracy', cv=3, verbose=0, n_jobs=5)\n",
    "\n",
    "# Fit the grid\n",
    "grid.fit(X_train, y_train)\n",
    "# Predictions\n",
    "y = grid.predict(X_test)\n",
    "print(\"-- Execution time: %s seconds ---\" % (time.time() - start_time))\n",
    "print(classification_report(y, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### XGB ############\n",
    "warnings.filterwarnings('ignore')\n",
    "start_time = time.time()\n",
    "# Create lemmatizer using spacy\n",
    "lemmatizer = spacy.lang.en.English()\n",
    "\n",
    "# Define Model\n",
    "xgb_model = XGBClassifier(learning_rate =0.1, n_estimators=140, max_depth=5, min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8, objective= 'binary:logistic', scale_pos_weight=1, seed=27) \n",
    "\n",
    "\n",
    "# Define custom Tokenizer\n",
    "def custom_tokenizer(doc):\n",
    "    tokens = lemmatizer(doc)\n",
    "    return([token.lemma_ for token in tokens if not token.is_punct])\n",
    "\n",
    "# Define Vectorizer\n",
    "vectorizer = TfidfVectorizer(tokenizer=custom_tokenizer, stop_words='english',\n",
    "                             lowercase=True, use_idf=True, max_df=2,\n",
    "                             min_df=2, norm='l2', smooth_idf=True, ngram_range=(1, 2))\n",
    "\n",
    "# Define Pipeline and Feature Union\n",
    "pipeline = Pipeline([\n",
    "    # Use Feature Union to combine features from the Tweet and other features gathered\n",
    "    ('union', FeatureUnion(\n",
    "        transformer_list=[\n",
    "            # Pipeline for text\n",
    "            ('tweet', Pipeline([\n",
    "                ('selector', DataSelector(key='text')),\n",
    "                ('vectidf', vectorizer),\n",
    "                ('svd', TruncatedSVD(1000)),\n",
    "                ('norm',Normalizer(copy=False))\n",
    "                                \n",
    "            ])),\n",
    "            \n",
    "            # Pipeline for getting other features\n",
    "            ('other', Pipeline([\n",
    "                ('seclector', DataSelector(key='other'))\n",
    "             ])),\n",
    "        ],\n",
    "                       \n",
    "    )),\n",
    "    # Use Logistic Regression Classifier\n",
    "    ('xgb', xgb_model)\n",
    "])\n",
    "\n",
    "# Grid Search\n",
    "parameters = {\n",
    "                \n",
    "                'xgb__n_jobs':[20],\n",
    "                'union__transformer_weights':[{'tweet':0.5, 'other':0.5},{'tweet':0.2, 'other':0.8},{'tweet':0.8, 'other':0.2}]\n",
    "               \n",
    "              }\n",
    "\n",
    "grid = GridSearchCV(pipeline, parameters, scoring='accuracy', cv=3, verbose=0, n_jobs=5)\n",
    "\n",
    "# Fit the grid\n",
    "grid.fit(X_train, y_train)\n",
    "# Predictions\n",
    "y = grid.predict(X_test)\n",
    "print(\"-- Execution time: %s seconds ---\" % (time.time() - start_time))\n",
    "print(classification_report(y, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
